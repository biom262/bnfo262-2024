{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spot detection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal when processing MERFISH images (or any fluorescent microscopy image) is to identify the fluorescent spots in the image, which are the molecules of interest that have been tagged with fluorescent probes. The image we will be processing here is a 2048x2048 pixel 16-bit monochrome image. Each pixel is 108nm (nanometers), so the entire image is about 220x220 microns. The image is saved as a numpy array, which is easy to load:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "ln -sfn ~/public/imgproc-spatialtx/spatialtx/* ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "img = np.load(\"merfish_image.npy\")\n",
    "print(img.shape)\n",
    "img"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the standard matplotlib plotting library to view images, using the `vmax` parameter to adjust the contrast:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(dpi=200)\n",
    "plt.imshow(img, cmap=\"gray\", vmax=3000)\n",
    "plt.axis(\"off\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can zoom in by plotting a smaller portion of the image to get a better view of the fluorescent spots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=200)\n",
    "plt.imshow(img[800:1200, 1000:1400], cmap=\"gray\", vmax=3000)\n",
    "plt.axis(\"off\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step of processing this image will be to use a high-pass filter. This is a standard image processing tool that reduces the background while highlighting the signal. This filter is done by subtracting a blurred version of the image from itself. The image is blurred by taking each pixel and adding a fraction of its value to the surrounding pixels following a gaussian distribution. The value `sigma` is the standard deviation of the gaussian curve, while the `window_size` is the size of the box around each pixel that the blurring process is allowed to modify. This should be large enough so that the gaussian curve of the given `sigma` becomes low enough to not make any significant difference. The formula below is a good way to determine this window size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "sigma = 3\n",
    "window_size = int(2 * np.ceil(2 * sigma) + 1)\n",
    "blurred_img = cv2.GaussianBlur(img, (window_size, window_size), sigma, borderType=cv2.BORDER_REPLICATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=200)\n",
    "plt.imshow(blurred_img, cmap=\"gray\", vmax=3000)\n",
    "plt.axis(\"off\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_img = img - blurred_img\n",
    "filtered_img[blurred_img > img] = 0  # What do you think is the purpose of this line? (hint: what data type are the values stored in the image?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, dpi=200, figsize=(8,4))\n",
    "ax[0].imshow(filtered_img, cmap=\"gray\", vmax=1000)\n",
    "ax[0].axis(\"off\")\n",
    "ax[1].imshow(filtered_img[800:1200, 1000:1400], cmap=\"gray\", vmax=1000)\n",
    "ax[1].axis(\"off\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this has made the fluorescent spots much clearer and reduced the background. \n",
    "\n",
    "**Try re-running the above steps with different values of `sigma` to see how it changes the result.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've pre-processed the image, it's time to decide where the actual fluorescent spots are. There are many different ways to do this, and it is an active area of research in the field. Here, we will be using a relatively simple method called adaptive thresholding. A thresholding method involves setting a pixel value for which all pixels above that value will be kept as spots, while those below are removed. Adaptive thresholding sets this cut-off value per pixel based on the surrounding, allowing it to adapt to varying levels of background and noise throughout the image. The key paramater for the function is the last one set at `-2`. This is a constant that is subtracted from the mean surrounding each pixel to determine the threshold. Changing this can have a huge affect on the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newimg = ((filtered_img / np.max(filtered_img)) * 255).astype(np.uint8) # The adaptiveThredhold function only works on 8-bit images\n",
    "binary_img = cv2.adaptiveThreshold(newimg, 1, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 23, -2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, dpi=200, figsize=(8,4))\n",
    "ax[0].imshow(binary_img, cmap=\"gray\")\n",
    "ax[0].axis(\"off\")\n",
    "ax[1].imshow(binary_img[800:1200, 1000:1400], cmap=\"gray\")\n",
    "ax[1].axis(\"off\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Change the `-2` above to some different values to see how it affects the result.** This value may require some manual fine-tuning from experiment to experiment. Next we will turn this binary image into a table of spots. We do this by first using the `label` function which will re-number each connected group of `1`s in the binary image so that they can be uniquely identified. Here is an example of how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import measure\n",
    "\n",
    "example = np.array([\n",
    "    [0, 0, 0, 0, 0, 0],\n",
    "    [0, 1, 1, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 1, 0],\n",
    "    [0, 0, 0, 0, 1, 0],\n",
    "    [0, 1, 0, 0, 0, 0],\n",
    "    [0, 1, 1, 0, 1, 0]\n",
    "])\n",
    "measure.label(example)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we use the `regionprops_table` function to extract some properties of the spots. The ones we are interested in are the area in pixels and the position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import measure\n",
    "import pandas as pd\n",
    "\n",
    "label_img = measure.label(binary_img)\n",
    "spots = pd.DataFrame(measure.regionprops_table(label_img, properties=(\"label\", \"area\", \"centroid\"))).set_index(\"label\").rename(columns={\"centroid-0\": \"x\", \"centroid-1\": \"y\"})\n",
    "spots"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of our spots are only a single pixel, which may just be noise. We will filter these out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spots = spots[spots.area > 1].copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can visualize the positions of these spots on top of our image to see if the result looks reasonable. We want to see that we aren't missing any obvious bright spots while also not marking anything that looks like noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=200)\n",
    "plt.imshow(filtered_img, cmap=\"gray\", vmax=1000)\n",
    "plt.scatter(spots.y, spots.x, facecolors=\"none\", edgecolors=\"tab:red\")  # We have to swap x and y here, it's just a quirk of matplotlib's coordinate systems\n",
    "plt.xlim(800, 1200)\n",
    "plt.ylim(1000, 1400)\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This doesn't look too bad. When we used the `regionprops_table` function above, we could have also specified the `intensity_image` parameter with either our original image or the highpass-filtered image, which would allow us to add additional information to the table such as the mean brightness/intensity of each spot. This could be used to further filter the spots. **Try to modify the above procedure using the `intensity_image` parameter of `regionprops_table`.**\n",
    "\n",
    "In classic smFISH (single-molecule FISH) each of these spots we've identified would be a transcript of a specific gene. However with MERFISH, these spots represent many different genes. We would have many of these images where a different set of genes fluoresce in each one. By finding the spots in all these images and then looking for spots that appear in the same position on different images, we can determine that spot's gene identity based on the coding scheme that has been designed for the experiment. For example, we might find that a spot appears in the same location in the first, third, fourth, and eigth image. While the spots could be multiple genes in each of those images independently, only a single gene will be in the intersection of the sets of genes for these four images.\n",
    "\n",
    "While the image we worked with above is a real MERFISH image, for the sake of this exercise we will pretend all the spots are transcripts of just a single gene."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell segmentation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've identified the spots in the image (remember, these spots are RNA transcripts), we want to assign each of them to cells so that we can get the expression level of this gene in each cell. To do this, we need to perform cell segmentation, which is the process of determining the borders of cells in an image. While the cells can be seen in the MERFISH image we just worked with, we will use a separate image to do this where rather than attaching fluorescent probes to specific RNAs of interest, polyT probes have been used which attach to the polyA tails of all RNAs. This gives us much brighter cells with clearer borders that help with the cell segmentation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.load(\"cell_staining.npy\")\n",
    "plt.figure(dpi=200)\n",
    "plt.imshow(img, cmap=\"gray\", vmax=15000)\n",
    "plt.axis(\"off\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do the segmentation, we will use a state of the art algorithm called cellpose, which is a deep neural network trained on many different cell images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cellpose import models\n",
    "model = models.Cellpose(model_type=\"cyto2\")\n",
    "mask, _, _, _ = model.eval(img, channels=[0, 0], diameter=80, cellprob_threshold=-4, flow_threshold=1.25)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The segmentation mask that is returned is very similar to the labeled spot image we produced above. The pixels belonging to each cell are given a distinct number, while `0` indicates pixels not within any cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=200)\n",
    "plt.imshow(img, cmap=\"gray\", vmax=15000)\n",
    "plt.contour(mask, [x+0.5 for x in np.unique(mask)], colors=\"tab:blue\", linewidths=0.5)\n",
    "plt.axis(\"off\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some artifacts that we can filter by removing cells containing a small number of pixels. **How would you remove cells with less than 2500 pixels? (Hint: The `regionprops_table` function we used before might be useful)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove this cell for students\n",
    "# There are other ways they might think to do this, e.g. using np.unique with return_counts=True\n",
    "sizes = pd.DataFrame(measure.regionprops_table(mask, properties=[\"label\", \"area\"]))\n",
    "mask[np.isin(mask, sizes[sizes.area < 2500].label)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=200)\n",
    "plt.imshow(img, cmap=\"gray\", vmax=15000)\n",
    "plt.contour(mask, [x+0.5 for x in np.unique(mask)], colors=\"tab:blue\", linewidths=0.5)\n",
    "plt.axis(\"off\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can assign the spots we identified to these cells. Because the pixel value in the cell segmentation mask is a unique identifier for the cell it belongs to, we can do this easily by indexing the mask array with the coordinates of each spot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spots[\"cell_id\"] = mask[spots.x.astype(int), spots.y.astype(int)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can confirm this worked by plotting the barcodes assigned to cells in a different color than those outside cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spots_in_cells = spots[spots.cell_id > 0]\n",
    "spots_outside_cells = spots[spots.cell_id == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=200)\n",
    "plt.imshow(img, cmap=\"gray\", vmax=15000)\n",
    "plt.scatter(spots_in_cells.y, spots_in_cells.x, s=2, c=\"tab:blue\")\n",
    "plt.scatter(spots_outside_cells.y, spots_outside_cells.x, s=2, c=\"tab:red\")\n",
    "plt.axis(\"off\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single-cell analysis of MERFISH"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a real MERFISH experiment, the above process would need to be repeated for hundreds of images and composed together to create a cell by gene matrix for an entire section of tissue. We will start with a pre-constructed cell by gene matrix from a heart section (Happy Valentine's Day!) to see how the spatial information can be used in downstream analysis. We will use the scanpy package and do count normalization and clustering exactly the same as a scRNA-seq experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "\n",
    "adata = sc.read_csv(\"cell_by_gene.csv\", first_column_names=True)\n",
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pp.calculate_qc_metrics(adata, percent_top=None, inplace=True)\n",
    "adata.obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pp.filter_cells(adata, min_counts=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pp.normalize_total(adata)\n",
    "sc.pp.log1p(adata)\n",
    "sc.pp.pca(adata)\n",
    "sc.pp.neighbors(adata)\n",
    "sc.tl.leiden(adata)\n",
    "sc.tl.umap(adata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(adata, color=\"leiden\", legend_loc=\"on data\", legend_fontoutline=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our clustering and UMAP, so let's add the spatial information to our scanpy object. We have a separate table of cell metadata that contains the spatial coordinates of each cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_metadata = pd.read_csv(\"cell_metadata.csv\", index_col=0)\n",
    "cell_metadata"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just need to add the spatial matrix to the `obsm` section of the scanpy object. `obsm` is for storing matrices where each row is a cell but the number of columns is arbitrary. Scanpy stores the PCA and UMAP matrices in `obsm`, for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.obsm[\"spatial\"] = cell_metadata.loc[adata.obs_names.astype(int)][[\"x\", \"y\"]].to_numpy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `sc.pl.embedding` to plot the data spatially. This function takes a `basis` parameter, which is the name of the matrix in `obsm` to use for coordinates. (This function has the same parameters as `sc.pl.umap`, in fact `sc.pl.umap` just calls `sc.pl.embedding` behind the scenes with `basis=\"UMAP\"`)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.embedding(adata, basis=\"spatial\", color=\"leiden\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see that many of the clusters we got by clustering just from gene expression information turn out to have very distinct spatial patterns. We can also plot genes and see their spatial patterns. IRX3, for example, is a known marker for trabecular cardiomyocytes, which is a cell type that is represented by cluster 3 in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.embedding(adata, basis=\"spatial\", color=\"IRX3\", cmap=\"Reds\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To look further at the spatial aspect of our data, we can use a library called Squidpy, which is a project that is building spatial transcriptomics analysis tools on top of scanpy. The first thing we will do is build a graph of spatial neighbors, which is information that can be used for a few different analyses. The spatial graph is a representation of which cells are within a specified radius of other cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import squidpy as sq\n",
    "\n",
    "sq.gr.spatial_neighbors(adata, coord_type=\"generic\", radius=150)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing we can do with the spatial neighbors graph is calculate neighborhood enrichment. This tells us which cluster/celltypes tend to appear near to each other and can indicate cell types that may interact with one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sq.gr.nhood_enrichment(adata, cluster_key=\"leiden\")\n",
    "sq.pl.nhood_enrichment(adata, cluster_key=\"leiden\", method=\"ward\", cmap=\"coolwarm\", vmax=300, vmin=-300)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see there are some groups of clusters, such as 6, 13, and 4, that are often found together. We can plot some of these groups to see where they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(ncols=3, figsize=(12,4))\n",
    "sc.pl.embedding(adata, basis=\"spatial\", color=\"leiden\", groups=[\"6\", \"13\", \"5\"], ax=ax[0], show=False)\n",
    "sc.pl.embedding(adata, basis=\"spatial\", color=\"leiden\", groups=[\"0\", \"7\", \"10\"], ax=ax[1], show=False)\n",
    "sc.pl.embedding(adata, basis=\"spatial\", color=\"leiden\", groups=[\"8\", \"9\"], ax=ax[2], show=False)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also find genes that have spatially distinct expression. Here we will use Moran's *I* auto-correlation and plot the top 6 genes based on this score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sq.gr.spatial_autocorr(adata, mode=\"moran\")\n",
    "adata.uns[\"moranI\"].head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.embedding(adata, basis=\"spatial\", color=[\"MYH7\", \"MYH6\", \"LBH\", \"COL2A1\", \"PAM\", \"IRX3\"], ncols=3, cmap=\"Reds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3d8e2bb359ff055586e40e51c4113df09cb3b64ae67be6ca96cbedae585306fb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
